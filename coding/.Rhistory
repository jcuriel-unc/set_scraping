install.packages("redist")
6+6+10+3+10+4+3+2+4+4
52+76
10+5+8+7+14+7+3+3+5+5
sort(sample(seq(1,10,by=1), replace = F))
sort(sample(5,seq(1,10,by=1), replace = F))
?sample
sort(sample(seq(1,10,by=1),5, replace = F))
sort(sample(seq(1,14,by=1),7, replace = F))
(20*.15)
96*.15
85*.3
15+3+14.4+25.5
install.packages("RSelenium")
### load in packages
library(tidyverse)
library(RSelenium)
### create Rs driver object
rs_driver_obj <- rsDriver(browser = "chrome", chromever = "114.0.5735.99")
### create Rs driver object
rs_driver_obj <- rsDriver(browser = "chrome", chromever = "113.0.5672.127")
java_check
java_check()
install.packages("JavaGD")
java_check()
library(JavaGD)
java_check()
library(rJava)
remDr$open()
library(RSelenium) # need java development kit installed for this to work
remDr$open()
?rsDriver
rs_driver_obj <- rsDriver(browser = "internet explorer")
r = 3+7
r
library(wru)
### load in packages
library(tidyverse) ## for efficient cleaning of data frames
library(wru) ## for predicting race of profs
library(foreign) ## for reading in of csv files and such
library(rstudioapi) ## for efficient grabbing of working directory info
library(ggplot2) ## for cool plots
library(irr)
library(arm)
library(peRspective)
library(grid)
###setting directory
main_wd <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(main_wd)
### re read in the data from Gabe and update
rmp_ratings <- read.csv("thumbs_master_unc_data_comments.csv")
View(rmp_ratings)
### read in the UNC data
unc_df <- read.csv("faculty characteristics web scraping - unc_comments_koen.csv")
unc_df2 <- read.csv("unc_comment_data_with_persp.csv")
### looks like the data is missing the ID fields. We will see what we can do with the comment data.
nrow(unc_df2)
nrow(rmp_ratings)
names(unc_df2)
names(rmp_ratings)
### they are different lengths. Will need to see whats going on
unc_df3 <- merge(unc_df2, rmp_ratings , by="comment")
library(tidyverse) ## for efficient cleaning of data frames
library(wru) ## for predicting race of profs
library(foreign) ## for reading in of csv files and such
library(rstudioapi) ## for efficient grabbing of working directory info
library(ggplot2) ## for cool plots
library(irr)
library(arm)
library(peRspective)
library(grid)
###setting directory
main_wd <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(main_wd)
unc_df2 <- read.csv("unc_comment_data_with_persp.csv")
### re read in the data from Gabe and update
rmp_ratings <- read.csv("final_thumbs_master_unc_data_comments.csv")
head(rmp_ratings)
nrow(rmp_ratings)
### looks like the data is missing the ID fields. We will see what we can do with the comment data.
nrow(unc_df2)
nrow(rmp_ratings)
sort(unique(rmp_ratings$row))
names(rmp_ratings)
names(unc_df2)
sort(unique(rmp_ratings$row))
rmp_ratings <- subset(rmp_ratings, select=-c(comment,difficulty_of_class,quality_of_class))
### they are different lengths. Will need to see whats going on
unc_df3 <- merge(unc_df2, rmp_ratings , by = c("college","prof_firstname","prof_lastname","row"))
### got 1360; not everything made it; why?
unc_miss <- merge(unc_df2, rmp_ratings , by = c("college","prof_firstname","prof_lastname","row"), all.x=T)
unc_miss <- subset(unc_miss, is.na(quality_of_class)==T)
### got 1360; not everything made it; why?
unc_miss <- merge(unc_df2, rmp_ratings , by = c("college","prof_firstname","prof_lastname","row"), all.x=T)
unc_miss <- subset(unc_miss, is.na(thumbs_down)==T)
View(unc_miss)
write.csv(unc_df3,"final_thumbs_master_unc_data_comments_merged.csv",row.names = F)
library(tidyverse) ## for efficient cleaning of data frames
library(wru) ## for predicting race of profs
library(foreign) ## for reading in of csv files and such
library(rstudioapi) ## for efficient grabbing of working directory info
library(ggplot2) ## for cool plots
library(irr)
library(arm)
###setting directory
main_wd <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(main_wd)
### Read in the csv data
local_files <-list.files(full.names=T)
local_files <- local_files[grepl(".csv",local_files)] ## subset to only include csv files
## read in the final data
gabe_coms <- read.csv(local_files[grepl("gabe", local_files) & grepl("final", local_files)])
koen_coms <- read.csv(local_files[grepl("koen", local_files) & grepl("final", local_files)])
dim(gabe_coms) ## 31 cols
is.na(gabe_coms$out_emo_lang)==T
sum(is.na(gabe_coms$out_emo_lang)==T)
sum(is.na(koen_coms$out_emo_lang)==T) # no missing
### lets create an aggregate index of the dims of interest
koen_coms$outrage_agg <- rowSums(koen_coms[,grepl("out_", colnames(koen_coms))])
koen_coms$personal_attack_agg <- rowSums(koen_coms[,grepl("pa_", colnames(koen_coms))])
koen_coms$prejudice_agg <- rowSums(koen_coms[,grepl("pb_", colnames(koen_coms))])
### now same for Gabe
gabe_coms$outrage_agg <- rowSums(gabe_coms[,grepl("out_", colnames(gabe_coms))])
gabe_coms$personal_attack_agg <- rowSums(gabe_coms[,grepl("pa_", colnames(gabe_coms))])
gabe_coms$prejudice_agg <- rowSums(gabe_coms[,grepl("pb_", colnames(gabe_coms))])
###create a df to store this info in
irr_df <- as.data.frame(matrix(nrow=26,ncol=6))
colnames(irr_df) <- c("item","kappa_score","z_score","p_value","gabe_num","koen_num")
### assign columns to items
irr_df$item <- colnames(koen_coms[9:ncol(koen_coms)])
names(koen_coms)
## get rid of thumbs data
gabe_coms <- subset(gabe_coms, select=-c(thumbs_up,thumbs_down))
koen_coms <- subset(koen_coms, select=-c(thumbs_up,thumbs_down))
## read in the final data
gabe_coms <- read.csv(local_files[grepl("gabe", local_files) & grepl("final", local_files)])
koen_coms <- read.csv(local_files[grepl("koen", local_files) & grepl("final", local_files)])
### since dealing with the final data, should be good to go.
### now subset to include only first 1100
#gabe_coms <- gabe_coms[1:1100,]
#koen_coms <- koen_coms[1:1100,]
dim(gabe_coms) ## 33 cols, 2800 rows
sum(is.na(gabe_coms$out_emo_lang)==T) # no missing
sum(is.na(koen_coms$out_emo_lang)==T) # no missing
# gabe_coms$out_emo_lang[is.na(gabe_coms$out_emo_lang)==T] <- 0
### lets create an aggregate index of the dims of interest
koen_coms$outrage_agg <- rowSums(koen_coms[,grepl("out_", colnames(koen_coms))])
koen_coms$personal_attack_agg <- rowSums(koen_coms[,grepl("pa_", colnames(koen_coms))])
koen_coms$prejudice_agg <- rowSums(koen_coms[,grepl("pb_", colnames(koen_coms))])
### now same for Gabe
gabe_coms$outrage_agg <- rowSums(gabe_coms[,grepl("out_", colnames(gabe_coms))])
gabe_coms$personal_attack_agg <- rowSums(gabe_coms[,grepl("pa_", colnames(gabe_coms))])
gabe_coms$prejudice_agg <- rowSums(gabe_coms[,grepl("pb_", colnames(gabe_coms))])
## get rid of thumbs data
gabe_coms <- subset(gabe_coms, select=-c(thumbs_up,thumbs_down))
koen_coms <- subset(koen_coms, select=-c(thumbs_up,thumbs_down))
### good; note that the rows of coding comments starts at 9
### We should be able to loop through all of these
###create
###create a df to store this info in
irr_df <- as.data.frame(matrix(nrow=26,ncol=6))
colnames(irr_df) <- c("item","kappa_score","z_score","p_value","gabe_num","koen_num")
### assign columns to items
irr_df$item <- colnames(koen_coms[9:ncol(koen_coms)])
for (i in 1:nrow(irr_df)) { ## starting off with first col of interest
temp_df <- cbind(gabe_coms[,i+8],koen_coms[,i+8])
irr_score <- kappa2(temp_df, weight = "equal") ## note: kappa 2 is for categorical or ordinal data
## store in df
irr_df$kappa_score[i] <- irr_score$value
irr_df$z_score[i] <- irr_score$statistic
irr_df$p_value[i] <- irr_score$p.value
irr_df$gabe_num[i] <- sum(gabe_coms[i+8]) ## storing # of instances
irr_df$koen_num[i] <- sum(koen_coms[i+8])
}
colSums(gabe_coms[,9:31])
colSums(koen_coms[,9:31])
View(irr_df)
#### save the irr file
write.csv(irr_df, "interrel_final.csv", row.names=F)
### let's add these and then take the average
combined_matrix <- koen_coms[9:ncol(koen_coms)] + gabe_coms[9:ncol(gabe_coms)] ## adding together
#combined_matrix <- combined_matrix/2 ## finding average score
class(combined_matrix) ## good, still data frame
### find summary
summary(combined_matrix)
final_df <- cbind(koen_coms[,1:8], combined_matrix)
### let's clean a bit
## removing all upper case instances
final_df$comment <- str_replace_all(final_df$comment,final_df$prof_lastname,"")
final_df$comment <- str_replace_all(final_df$comment,final_df$prof_firstname,"")
## removing all title case
final_df$comment <- str_replace_all(final_df$comment,str_to_title(final_df$prof_lastname),"")
final_df$comment <- str_replace_all(final_df$comment,str_to_title(final_df$prof_firstname),"")
## removing all lower case
final_df$comment <- str_replace_all(final_df$comment,str_to_lower(final_df$prof_lastname),"")
final_df$comment <- str_replace_all(final_df$comment,str_to_lower(final_df$prof_firstname),"")
### now let's export as csv
write.csv(final_df, "text_cleaning_data/scored_rmp_data_osu_final.csv", row.names = F)
cor(final_df$constructive, final_df$outrage_agg)
cor(final_df$constructive, (final_df$outrage_agg+final_df$prejudice_agg+final_df$personal_attack_agg))
