install.packages("redist")
6+6+10+3+10+4+3+2+4+4
52+76
10+5+8+7+14+7+3+3+5+5
sort(sample(seq(1,10,by=1), replace = F))
sort(sample(5,seq(1,10,by=1), replace = F))
?sample
sort(sample(seq(1,10,by=1),5, replace = F))
sort(sample(seq(1,14,by=1),7, replace = F))
(20*.15)
96*.15
85*.3
15+3+14.4+25.5
install.packages("RSelenium")
### load in packages
library(tidyverse)
library(RSelenium)
### create Rs driver object
rs_driver_obj <- rsDriver(browser = "chrome", chromever = "114.0.5735.99")
### create Rs driver object
rs_driver_obj <- rsDriver(browser = "chrome", chromever = "113.0.5672.127")
java_check
java_check()
install.packages("JavaGD")
java_check()
library(JavaGD)
java_check()
library(rJava)
remDr$open()
library(RSelenium) # need java development kit installed for this to work
remDr$open()
?rsDriver
rs_driver_obj <- rsDriver(browser = "internet explorer")
r = 3+7
r
library(wru)
### load in packages
library(tidyverse) ## for efficient cleaning of data frames
library(wru) ## for predicting race of profs
library(foreign) ## for reading in of csv files and such
library(rstudioapi) ## for efficient grabbing of working directory info
library(ggplot2) ## for cool plots
library(irr)
library(arm)
library(peRspective)
library(grid)
###setting directory
main_wd <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(main_wd)
### re read in the data from Gabe and update
rmp_ratings <- read.csv("thumbs_master_unc_data_comments.csv")
View(rmp_ratings)
### read in the UNC data
unc_df <- read.csv("faculty characteristics web scraping - unc_comments_koen.csv")
unc_df2 <- read.csv("unc_comment_data_with_persp.csv")
### looks like the data is missing the ID fields. We will see what we can do with the comment data.
nrow(unc_df2)
nrow(rmp_ratings)
names(unc_df2)
names(rmp_ratings)
### they are different lengths. Will need to see whats going on
unc_df3 <- merge(unc_df2, rmp_ratings , by="comment")
library(tidyverse) ## for efficient cleaning of data frames
library(wru) ## for predicting race of profs
library(foreign) ## for reading in of csv files and such
library(rstudioapi) ## for efficient grabbing of working directory info
library(ggplot2) ## for cool plots
library(irr)
library(arm)
library(peRspective)
library(grid)
###setting directory
main_wd <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(main_wd)
unc_df2 <- read.csv("unc_comment_data_with_persp.csv")
### re read in the data from Gabe and update
rmp_ratings <- read.csv("final_thumbs_master_unc_data_comments.csv")
head(rmp_ratings)
nrow(rmp_ratings)
### looks like the data is missing the ID fields. We will see what we can do with the comment data.
nrow(unc_df2)
nrow(rmp_ratings)
sort(unique(rmp_ratings$row))
names(rmp_ratings)
names(unc_df2)
sort(unique(rmp_ratings$row))
rmp_ratings <- subset(rmp_ratings, select=-c(comment,difficulty_of_class,quality_of_class))
### they are different lengths. Will need to see whats going on
unc_df3 <- merge(unc_df2, rmp_ratings , by = c("college","prof_firstname","prof_lastname","row"))
### got 1360; not everything made it; why?
unc_miss <- merge(unc_df2, rmp_ratings , by = c("college","prof_firstname","prof_lastname","row"), all.x=T)
unc_miss <- subset(unc_miss, is.na(quality_of_class)==T)
### got 1360; not everything made it; why?
unc_miss <- merge(unc_df2, rmp_ratings , by = c("college","prof_firstname","prof_lastname","row"), all.x=T)
unc_miss <- subset(unc_miss, is.na(thumbs_down)==T)
View(unc_miss)
write.csv(unc_df3,"final_thumbs_master_unc_data_comments_merged.csv",row.names = F)
library(tidyverse) ## for efficient cleaning of data frames
library(wru) ## for predicting race of profs
library(foreign) ## for reading in of csv files and such
library(rstudioapi) ## for efficient grabbing of working directory info
library(ggplot2) ## for cool plots
library(irr)
library(arm)
###setting directory
main_wd <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(main_wd)
### Read in the csv data
local_files <-list.files(full.names=T)
local_files <- local_files[grepl(".csv",local_files)] ## subset to only include csv files
## read in the final data
gabe_coms <- read.csv(local_files[grepl("gabe", local_files) & grepl("final", local_files)])
koen_coms <- read.csv(local_files[grepl("koen", local_files) & grepl("final", local_files)])
dim(gabe_coms) ## 31 cols
is.na(gabe_coms$out_emo_lang)==T
sum(is.na(gabe_coms$out_emo_lang)==T)
sum(is.na(koen_coms$out_emo_lang)==T) # no missing
### lets create an aggregate index of the dims of interest
koen_coms$outrage_agg <- rowSums(koen_coms[,grepl("out_", colnames(koen_coms))])
koen_coms$personal_attack_agg <- rowSums(koen_coms[,grepl("pa_", colnames(koen_coms))])
koen_coms$prejudice_agg <- rowSums(koen_coms[,grepl("pb_", colnames(koen_coms))])
### now same for Gabe
gabe_coms$outrage_agg <- rowSums(gabe_coms[,grepl("out_", colnames(gabe_coms))])
gabe_coms$personal_attack_agg <- rowSums(gabe_coms[,grepl("pa_", colnames(gabe_coms))])
gabe_coms$prejudice_agg <- rowSums(gabe_coms[,grepl("pb_", colnames(gabe_coms))])
###create a df to store this info in
irr_df <- as.data.frame(matrix(nrow=26,ncol=6))
colnames(irr_df) <- c("item","kappa_score","z_score","p_value","gabe_num","koen_num")
### assign columns to items
irr_df$item <- colnames(koen_coms[9:ncol(koen_coms)])
names(koen_coms)
## get rid of thumbs data
gabe_coms <- subset(gabe_coms, select=-c(thumbs_up,thumbs_down))
koen_coms <- subset(koen_coms, select=-c(thumbs_up,thumbs_down))
## read in the final data
gabe_coms <- read.csv(local_files[grepl("gabe", local_files) & grepl("final", local_files)])
koen_coms <- read.csv(local_files[grepl("koen", local_files) & grepl("final", local_files)])
### since dealing with the final data, should be good to go.
### now subset to include only first 1100
#gabe_coms <- gabe_coms[1:1100,]
#koen_coms <- koen_coms[1:1100,]
dim(gabe_coms) ## 33 cols, 2800 rows
sum(is.na(gabe_coms$out_emo_lang)==T) # no missing
sum(is.na(koen_coms$out_emo_lang)==T) # no missing
# gabe_coms$out_emo_lang[is.na(gabe_coms$out_emo_lang)==T] <- 0
### lets create an aggregate index of the dims of interest
koen_coms$outrage_agg <- rowSums(koen_coms[,grepl("out_", colnames(koen_coms))])
koen_coms$personal_attack_agg <- rowSums(koen_coms[,grepl("pa_", colnames(koen_coms))])
koen_coms$prejudice_agg <- rowSums(koen_coms[,grepl("pb_", colnames(koen_coms))])
### now same for Gabe
gabe_coms$outrage_agg <- rowSums(gabe_coms[,grepl("out_", colnames(gabe_coms))])
gabe_coms$personal_attack_agg <- rowSums(gabe_coms[,grepl("pa_", colnames(gabe_coms))])
gabe_coms$prejudice_agg <- rowSums(gabe_coms[,grepl("pb_", colnames(gabe_coms))])
## get rid of thumbs data
gabe_coms <- subset(gabe_coms, select=-c(thumbs_up,thumbs_down))
koen_coms <- subset(koen_coms, select=-c(thumbs_up,thumbs_down))
### good; note that the rows of coding comments starts at 9
### We should be able to loop through all of these
###create
###create a df to store this info in
irr_df <- as.data.frame(matrix(nrow=26,ncol=6))
colnames(irr_df) <- c("item","kappa_score","z_score","p_value","gabe_num","koen_num")
### assign columns to items
irr_df$item <- colnames(koen_coms[9:ncol(koen_coms)])
for (i in 1:nrow(irr_df)) { ## starting off with first col of interest
temp_df <- cbind(gabe_coms[,i+8],koen_coms[,i+8])
irr_score <- kappa2(temp_df, weight = "equal") ## note: kappa 2 is for categorical or ordinal data
## store in df
irr_df$kappa_score[i] <- irr_score$value
irr_df$z_score[i] <- irr_score$statistic
irr_df$p_value[i] <- irr_score$p.value
irr_df$gabe_num[i] <- sum(gabe_coms[i+8]) ## storing # of instances
irr_df$koen_num[i] <- sum(koen_coms[i+8])
}
colSums(gabe_coms[,9:31])
colSums(koen_coms[,9:31])
View(irr_df)
#### save the irr file
write.csv(irr_df, "interrel_final.csv", row.names=F)
### let's add these and then take the average
combined_matrix <- koen_coms[9:ncol(koen_coms)] + gabe_coms[9:ncol(gabe_coms)] ## adding together
#combined_matrix <- combined_matrix/2 ## finding average score
class(combined_matrix) ## good, still data frame
### find summary
summary(combined_matrix)
final_df <- cbind(koen_coms[,1:8], combined_matrix)
### let's clean a bit
## removing all upper case instances
final_df$comment <- str_replace_all(final_df$comment,final_df$prof_lastname,"")
final_df$comment <- str_replace_all(final_df$comment,final_df$prof_firstname,"")
## removing all title case
final_df$comment <- str_replace_all(final_df$comment,str_to_title(final_df$prof_lastname),"")
final_df$comment <- str_replace_all(final_df$comment,str_to_title(final_df$prof_firstname),"")
## removing all lower case
final_df$comment <- str_replace_all(final_df$comment,str_to_lower(final_df$prof_lastname),"")
final_df$comment <- str_replace_all(final_df$comment,str_to_lower(final_df$prof_firstname),"")
### now let's export as csv
write.csv(final_df, "text_cleaning_data/scored_rmp_data_osu_final.csv", row.names = F)
cor(final_df$constructive, final_df$outrage_agg)
cor(final_df$constructive, (final_df$outrage_agg+final_df$prejudice_agg+final_df$personal_attack_agg))
library(tidyverse) ## for efficient cleaning of data frames
library(wru) ## for predicting race of profs
library(foreign) ## for reading in of csv files and such
library(rstudioapi) ## for efficient grabbing of working directory info
library(ggplot2) ## for cool plots
library(irr)
library(arm)
library(peRspective)
library(grid)
###setting directory
main_wd <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(main_wd)
### api key
pr_api <- "AIzaSyBcW836khRPsqS-Fkkm7P0LNfk4B98_sZ0"
### read in the UNC data
unc_df <- read.csv("faculty characteristics web scraping - unc_comments_koen.csv")
unc_df <- read.csv("faculty characteristics web scraping - unc_comments_gabe.csv")
### read in the UNC data
unc_df_k <- read.csv("faculty characteristics web scraping - unc_comments_koen.csv")
unc_df_g <- read.csv("faculty characteristics web scraping - unc_comments_gabe.csv")
### ensure same cols and first 500 rows
unc_df_k <- unc_df_k[1:500,]
### ensure same cols and first 500 rows
unc_df_k <- unc_df_k[1:500,]
unc_df_g <- unc_df_g[1:500,]
ncol(unc_df_k)==ncol(unc_df_k)
### now do the averaging and such for consistency.
names(unc_df_k)
### results matrix
unc_results <- unc_df_g[,10:29] + unc_df_k[,10:29]
unc_results
## now check the aggregation
unc_results$total_toxic <- rowSums(unc_results)
## merge with the id info from unc
unc_results_merge <- cbind(unc_df_g[,1:9],unc_results)
View(unc_results_merge)
### read in the OSU data, subsetted
rmp_df_persp_osu <- read.csv("sample_peRspective_coded342obs.csv")
### unc 500 obs
unc_df_persp <- read.csv("unc_comment_data_with_persp.csv")
head(unc_df_persp)
names(unc_df_persp)
names(unc_results_merge)
### unc 500 obs
unc_df_persp <- readRDS("perspective_unc_data.rds")
names(unc_df_persp)
unc_results_merge <- merge(unc_results_merge, unc_df_persp, by.x="row_id", by.y="text_id" )
## now let's get quantile of data
quantile(unc_results_merge$total_toxic, seq(0,1,by=0.05))
## would also be good to find correlation between the measures
cor(rowSums(unc_df_g[,10:29], rowSums(unc_df_k[,10:29])))
unc_df_g[,10:29]
k_agg <- rowSums(unc_df_k[,10:29])
g_agg <- rowSums(unc_df_g[,10:29])
## would also be good to find correlation between the measures
cor(k_agg,g_agg )
## very off; we'll need to check
quantile(k_agg, seq(0,1,by=0.05))
quantile(g_agg, seq(0,1,by=0.05))
k_agg
g_agg
### how does total toxicity relate to the scores?
cor(unc_results_merge$total_toxic,unc_results_merge$TOXICITY)
### how does total toxicity relate to the scores?
lm(unc_results_merge$total_toxic ~ unc_results_merge$TOXICITY)
### how does total toxicity relate to the scores?
mod1 <- lm(unc_results_merge$total_toxic ~ unc_results_merge$TOXICITY)
summary(mod1)
### how does total toxicity relate to the scores?
mod1 <- lm( unc_results_merge$TOXICITY ~ unc_results_merge$total_toxic)
summary(mod1)
### read in the UNC data
unc_df_k <- read.csv("faculty characteristics web scraping - unc_comments_koen.csv")
unc_df_g <- read.csv("faculty characteristics web scraping - unc_comments_gabe.csv")
### ensure same cols and first 500 rows
unc_df_k <- unc_df_k[1:500,]
unc_df_g <- unc_df_g[1:500,]
ncol(unc_df_k)==ncol(unc_df_k) # TRUE; good
## check the diffs in the fields
colSums(unc_df_k)
## check the diffs in the fields
colSums(unc_df_k[,10:29])
colSums(unc_df_g[,10:29])
## ICC here
?icc
## ICC here
toxic_icc <- icc(c(k_agg,g_agg), type="agreement")
## ICC here
toxic_icc <- icc(c(k_agg,g_agg))
toxic_icc
k_agg
g_agg
## ICC here
toxic_icc <- icc(t(c(k_agg,g_agg)))
## bind into matrix
test_mat_unc <- matrix(cbind(k_agg,g_agg))
dim(test_mat_unc)
## bind into matrix
test_mat_unc <- matrix(cbind(k_agg,g_agg),ncol=2)
dim(test_mat_unc)
## ICC here
toxic_icc <- icc(test_mat_unc)
toxic_icc
r1 <- round(rnorm(20, 10, 4))
r2 <- round(r1 + 10 + rnorm(20, 0, 2))
r3 <- round(r1 + 20 + rnorm(20, 0, 2))
icc(cbind(r1, r2, r3), "twoway")
## ICC here
toxic_icc <- icc(test_mat_unc, "twoway")
toxic_icc
## ICC here
toxic_icc <- icc(test_mat_unc, "oneway")
toxic_icc
## ICC here
toxic_icc <- icc(test_mat_unc, "oneway", "agreement")
toxic_icc
## check the diffs in the fields
colSums(unc_df_k[,10:29])
colSums(unc_df_g[,10:29])
### results matrix
unc_results_diffs <- unc_df_g[,10:29] - unc_df_k[,10:29]
### let's bind the data, and then subset if diff
unc_results_diffs2 <- cbind(unc_df_g[1:9], unc_results_diffs)
## create a col on num of diffs
unc_results_diffs2$total_diffs <- rowSums(dat[,] != 0)
## create a col on num of diffs
unc_results_diffs2$total_diffs <- rowSums(unc_results_diffs2[,] != 0)
View(unc_results_diffs2)
## create a col on num of diffs
unc_results_diffs2$total_diffs <- rowSums(unc_results_diffs2[,-c(1:9)] != 0)
View(unc_results_diffs2)
## create a col on num of diffs
unc_results_diffs2$total_diffs <- rowSums(unc_results_diffs2[,-c(1:9)] != 0) -1
unc_results_diffs2 <- subset(unc_results_diffs2, total_diffs != 0)
## that worked. Now let's get this exported
write.csv(unc_results_diffs2, "unc500diffs.csv", row.names = F)
table(unc_results_diffs2$total_diffs)
### read in the UNC data
unc_df_k <- read.csv("faculty characteristics web scraping - unc_comments_koen.csv")
unc_df_g <- read.csv("faculty characteristics web scraping - unc_comments_gabe.csv")
View(unc_df_k)
View(unc_df_g)
head(unc_df_k)
unc_df_k <- subset(unc_df_k, college  != "OHIO STATE UNIVERSITY" )
unc_df_g <- subset(unc_df_g, college  != "OHIO STATE UNIVERSITY" )
length(unique(unc_df_k$row_id))
length(unique(unc_df_g$row_id))
### now sort correctly
unc_df_g <- unc_df_g[order(unc_df_g$row_id), ]
unc_df_k <- unc_df_k[order(unc_df_k$row_id), ]
### limit to above threshold
unc_df_g <- subset(unc_df_g, above_threshold == 1)
unc_df_k <- subset(unc_df_k, above_threshold == 1)
### read in the UNC data
unc_df_k <- read.csv("faculty characteristics web scraping - unc_comments_koen.csv")
unc_df_g <- read.csv("faculty characteristics web scraping - unc_comments_gabe.csv")
unc_df_k <- subset(unc_df_k, college  != "OHIO STATE UNIVERSITY" )
unc_df_g <- subset(unc_df_g, college  != "OHIO STATE UNIVERSITY" )
### now sort correctly
unc_df_g <- unc_df_g[order(unc_df_g$row_id), ]
unc_df_k <- unc_df_k[order(unc_df_k$row_id), ]
### limit to above threshold
unc_df_g <- subset(unc_df_g, above_threshold == 1)
unc_df_k <- subset(unc_df_k, above_threshold == 1)
names(unc_df_g)
names(unc_df_k)
## drop thumbs data
unc_df_g <- subset(unc_df_g, select=-c(thumbs_up,thumbs_down))
### ensure same cols and first 500 rows
ncol(unc_df_k)==ncol(unc_df_k) # TRUE; good
## check the diffs in the fields
colSums(unc_df_k[,10:29])
colSums(unc_df_g[,10:29])
### results matrix
unc_results_diffs <- unc_df_g[,10:29] - unc_df_k[,10:29]
### let's bind the data, and then subset if diff
unc_results_diffs2 <- cbind(unc_df_g[1:9], unc_results_diffs)
## create a col on num of diffs
unc_results_diffs2$total_diffs <- rowSums(unc_results_diffs2[,-c(1:9)] != 0) -1
unc_results_diffs2 <- subset(unc_results_diffs2, total_diffs != 0)
k_agg <- rowSums(unc_df_k[,10:29])
g_agg <- rowSums(unc_df_g[,10:29])
## bind into matrix
test_mat_unc <- matrix(cbind(k_agg,g_agg),ncol=2)
## ICC here
toxic_icc <- icc(test_mat_unc, "oneway", "agreement")
toxic_icc
### let's bind the data, and then subset if diff
unc_results_diffs2 <- cbind(unc_df_g[1:9], unc_results_diffs)
## create a col on num of diffs
unc_results_diffs2$total_diffs <- rowSums(unc_results_diffs2[,-c(1:9)] != 0) -1
unc_results_diffs2 <- subset(unc_results_diffs2, total_diffs != 0)
## get table of diffs
table(unc_results_diffs2$total_diffs)
### let's bind the data, and then subset if diff
unc_results_diffs2 <- cbind(unc_df_g[1:9], unc_results_diffs)
## create a col on num of diffs
unc_results_diffs2$total_diffs <- rowSums(unc_results_diffs2[,-c(1:9)] != 0)
unc_results_diffs2 <- subset(unc_results_diffs2, total_diffs != 0)
## get table of diffs
table(unc_results_diffs2$total_diffs)
## that worked. Now let's get this exported
write.csv(unc_results_diffs2, "unc500diffs.csv", row.names = F)
## that worked. Now let's get this exported
write.csv(unc_results_diffs2, "unc500diffs.csv", row.names = F)
