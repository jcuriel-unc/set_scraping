{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "689ab111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost==1.7.5 in c:\\users\\j-curiel\\anaconda3\\lib\\site-packages (1.7.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\j-curiel\\anaconda3\\lib\\site-packages (from xgboost==1.7.5) (1.21.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\j-curiel\\anaconda3\\lib\\site-packages (from xgboost==1.7.5) (1.9.1)\n"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "##### Template code and instructions on the basics of machine learning#\n",
    "##########################################################################\n",
    "### Step 1: install packages - \n",
    "##### packages are necessary to install and load, given that they have the built in functions necessary to run complex tasks. \n",
    "## They effectively act as one of the most crucial time saving activities that would otherwise lead to overly long and \n",
    "## duplicative scripts. \n",
    "\n",
    "### note: you are not expected to remember all of these; just for the best to copy and paste these sections \n",
    "\n",
    "## install pkgs \n",
    "import sys\n",
    "!{sys.executable} -m pip install xgboost==1.7.5 # note: needed since it looks like anaconda installs an earlier version \n",
    "# of the package, which is not helpful. 1.7.5 allows for the categorical data of interest to be used. \n",
    "\n",
    "# !{sys.executable} -m pip install requests #; this code here can be used to install packages on anaconda/jupyter notebook \n",
    "### I believe the below should be installed by default \n",
    "import requests # web scraping \n",
    "from bs4 import BeautifulSoup # for web scraping \n",
    "import itertools # for efficient operation of loops \n",
    "import pandas as pd # necessary for reading in, creating, and manipulating data frames \n",
    "import csv ## for importing/exporting csvs \n",
    "import glob ## for finding files in path\n",
    "import re\n",
    "import numpy as np\n",
    "### THe ML packages \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# sklearn packages\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# nltk packages; these are for the purpose of cleaning text, which will be crucial \n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b14bda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat      cut color clarity  depth  table  price     x     y     z\n",
       "0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
       "1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
       "2   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
       "3   0.29  Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
       "4   0.31     Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Section 1: numeric machine learning \n",
    "# link: https://www.datacamp.com/tutorial/xgboost-in-python\n",
    "\n",
    "diamonds = sns.load_dataset(\"diamonds\") # load in the diamonds data set from the sns pkg \n",
    "\n",
    "diamonds.head() ## look at the top rows for the data set, now named diamonds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f2a645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53940, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### take a look at the rows and cols of the data set \n",
    "diamonds.shape # (53940, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9daf9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.797940</td>\n",
       "      <td>61.749405</td>\n",
       "      <td>57.457184</td>\n",
       "      <td>3932.799722</td>\n",
       "      <td>5.731157</td>\n",
       "      <td>5.734526</td>\n",
       "      <td>3.538734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.474011</td>\n",
       "      <td>1.432621</td>\n",
       "      <td>2.234491</td>\n",
       "      <td>3989.439738</td>\n",
       "      <td>1.121761</td>\n",
       "      <td>1.142135</td>\n",
       "      <td>0.705699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>950.000000</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>4.720000</td>\n",
       "      <td>2.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>61.800000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>2401.000000</td>\n",
       "      <td>5.700000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>3.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.040000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>5324.250000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>4.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.010000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>18823.000000</td>\n",
       "      <td>10.740000</td>\n",
       "      <td>58.900000</td>\n",
       "      <td>31.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              carat         depth         table         price             x  \\\n",
       "count  53940.000000  53940.000000  53940.000000  53940.000000  53940.000000   \n",
       "mean       0.797940     61.749405     57.457184   3932.799722      5.731157   \n",
       "std        0.474011      1.432621      2.234491   3989.439738      1.121761   \n",
       "min        0.200000     43.000000     43.000000    326.000000      0.000000   \n",
       "25%        0.400000     61.000000     56.000000    950.000000      4.710000   \n",
       "50%        0.700000     61.800000     57.000000   2401.000000      5.700000   \n",
       "75%        1.040000     62.500000     59.000000   5324.250000      6.540000   \n",
       "max        5.010000     79.000000     95.000000  18823.000000     10.740000   \n",
       "\n",
       "                  y             z  \n",
       "count  53940.000000  53940.000000  \n",
       "mean       5.734526      3.538734  \n",
       "std        1.142135      0.705699  \n",
       "min        0.000000      0.000000  \n",
       "25%        4.720000      2.910000  \n",
       "50%        5.710000      3.530000  \n",
       "75%        6.540000      4.040000  \n",
       "max       58.900000     31.800000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### get the summaries of the numeric variables \n",
    "diamonds.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7e45145",
   "metadata": {},
   "outputs": [],
   "source": [
    "### now clean the data frame \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract feature and target arrays\n",
    "X, y = diamonds.drop('price', axis=1), diamonds[['price']]\n",
    "## what's going on above? The X, y are saying to create two objects, X and y. The part before the comma relates to X. The \n",
    "# portion after the comma refers to y, or the second object. \n",
    "## The \"diamonds.drop('price', axis=1)\" is creating a data frame with the .drop command, which drops the column titled \"price\"\n",
    "# The second part after the comma is in turn just creating a data frame from just the column \"price\", which will now be \n",
    "# located in the object y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "943bbfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text features\n",
    "## first, create an object \"cats\", which is a subset of the data frame X. The exclude option using the np package (numpy)\n",
    "# tells the command to drop all columns of the numeric variety. The .columns.tolist in turn is a command to turn into a list. \n",
    "cats = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# Convert to Pandas category; this makes it easier on memory \n",
    "for col in cats:\n",
    "   X[col] = X[col].astype('category')\n",
    "## note that this combines the list in cats with our previous object, X. We are saying that for the columns seen in the cats \n",
    "# object, whenever the column text in X matches with that of the column/list in cats, then it will change the variable in X \n",
    "# into a categorical one (i.e. factor) as opposed to a string var. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1c7404d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "carat       float64\n",
       "cut        category\n",
       "color      category\n",
       "clarity    category\n",
       "depth       float64\n",
       "table       float64\n",
       "x           float64\n",
       "y           float64\n",
       "z           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## take a look at the types now \n",
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5333befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data; follows the same syntax as box 6 via the commas \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1337)\n",
    "## what this code is doing is creating train and test objects on the IVs and DV. The train_test_split does this in a manner \n",
    "# where the train and test sets by var type will match, and the random_state is the seed, which will ensure that we will get \n",
    "# the same randomized selection everytime that we run the code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd25697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### where we import the xgboost pkg and run an ml algorithm \n",
    "import xgboost as xgb\n",
    "\n",
    "# Create regression matrices\n",
    "dtrain_reg = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "dtest_reg = xgb.DMatrix(X_test, y_test, enable_categorical=True)\n",
    "#dtrain_reg = xgb.DMatrix(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94ca1088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"gpu_hist\"} ## will feed into model fit testing of the machine \n",
    "#learning \n",
    "\n",
    "n = 100\n",
    "model = xgb.train(\n",
    "   params=params,\n",
    "   dtrain=dtrain_reg, \n",
    "   num_boost_round=n,\n",
    ")\n",
    "\n",
    "\n",
    "### example model fit stats \n",
    "\n",
    "#mse = np.mean((actual - predicted) ** 2) ## mean squared error \n",
    "#rmse = np.sqrt(mse) # root mean squared error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be2b7867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 612.978  , 6777.0317 ,  793.95917, ..., 1389.201  ,  538.1163 ,\n",
       "       1552.1204 ], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### let's evaluate the model \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "preds = model.predict(dtest_reg) # storing the prediction from the test model into the preds object \n",
    "preds ## stores the predicted prices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d25f15b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of the base model: 537.665\n"
     ]
    }
   ],
   "source": [
    "rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "\n",
    "print(f\"RMSE of the base model: {rmse:.3f}\") ##gets the root mean squared error. compares the models \n",
    "#RMSE of the base model: 543.203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7015f36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2384</th>\n",
       "      <td>561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18224</th>\n",
       "      <td>7397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36341</th>\n",
       "      <td>936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31026</th>\n",
       "      <td>749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7125</th>\n",
       "      <td>4173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16559</th>\n",
       "      <td>6617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8278</th>\n",
       "      <td>4381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41489</th>\n",
       "      <td>1232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42954</th>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45157</th>\n",
       "      <td>1654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13485 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       price\n",
       "2384     561\n",
       "18224   7397\n",
       "36341    936\n",
       "31026    749\n",
       "7125    4173\n",
       "...      ...\n",
       "16559   6617\n",
       "8278    4381\n",
       "41489   1232\n",
       "42954    506\n",
       "45157   1654\n",
       "\n",
       "[13485 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test # the y var from the test model, the applied predictions from the training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1f01288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:3971.96772\tvalidation-rmse:3960.17185\n",
      "[1]\ttrain-rmse:2838.56612\tvalidation-rmse:2828.86685\n",
      "[2]\ttrain-rmse:2054.95156\tvalidation-rmse:2049.31424\n",
      "[3]\ttrain-rmse:1515.40611\tvalidation-rmse:1514.02909\n",
      "[4]\ttrain-rmse:1153.60406\tvalidation-rmse:1155.57009\n",
      "[5]\ttrain-rmse:913.97875\tvalidation-rmse:922.84190\n",
      "[6]\ttrain-rmse:762.16906\tvalidation-rmse:775.54397\n",
      "[7]\ttrain-rmse:668.07790\tvalidation-rmse:684.71596\n",
      "[8]\ttrain-rmse:609.09360\tvalidation-rmse:629.87792\n",
      "[9]\ttrain-rmse:574.19616\tvalidation-rmse:598.06631\n",
      "[10]\ttrain-rmse:552.31608\tvalidation-rmse:578.31728\n",
      "[11]\ttrain-rmse:537.87321\tvalidation-rmse:565.67839\n",
      "[12]\ttrain-rmse:528.33436\tvalidation-rmse:559.34358\n",
      "[13]\ttrain-rmse:520.74475\tvalidation-rmse:554.74550\n",
      "[14]\ttrain-rmse:515.04677\tvalidation-rmse:550.59145\n",
      "[15]\ttrain-rmse:511.36064\tvalidation-rmse:548.68574\n",
      "[16]\ttrain-rmse:504.59900\tvalidation-rmse:545.15130\n",
      "[17]\ttrain-rmse:501.49288\tvalidation-rmse:542.99614\n",
      "[18]\ttrain-rmse:495.88358\tvalidation-rmse:542.22918\n",
      "[19]\ttrain-rmse:491.81691\tvalidation-rmse:539.63143\n",
      "[20]\ttrain-rmse:489.95163\tvalidation-rmse:538.56081\n",
      "[21]\ttrain-rmse:486.07198\tvalidation-rmse:537.84395\n",
      "[22]\ttrain-rmse:484.55606\tvalidation-rmse:537.54759\n",
      "[23]\ttrain-rmse:482.04784\tvalidation-rmse:537.89611\n",
      "[24]\ttrain-rmse:480.19216\tvalidation-rmse:537.75743\n",
      "[25]\ttrain-rmse:476.38540\tvalidation-rmse:538.03026\n",
      "[26]\ttrain-rmse:473.50174\tvalidation-rmse:538.34342\n",
      "[27]\ttrain-rmse:471.41729\tvalidation-rmse:538.43209\n",
      "[28]\ttrain-rmse:469.73956\tvalidation-rmse:537.81032\n",
      "[29]\ttrain-rmse:464.55022\tvalidation-rmse:539.14621\n",
      "[30]\ttrain-rmse:462.67901\tvalidation-rmse:538.66939\n",
      "[31]\ttrain-rmse:460.81301\tvalidation-rmse:538.47225\n",
      "[32]\ttrain-rmse:459.72722\tvalidation-rmse:538.18462\n",
      "[33]\ttrain-rmse:457.74964\tvalidation-rmse:537.94505\n",
      "[34]\ttrain-rmse:456.12669\tvalidation-rmse:537.70465\n",
      "[35]\ttrain-rmse:454.54985\tvalidation-rmse:537.40792\n",
      "[36]\ttrain-rmse:451.95202\tvalidation-rmse:537.60085\n",
      "[37]\ttrain-rmse:451.24764\tvalidation-rmse:537.39305\n",
      "[38]\ttrain-rmse:449.31446\tvalidation-rmse:537.35341\n",
      "[39]\ttrain-rmse:446.94345\tvalidation-rmse:536.55036\n",
      "[40]\ttrain-rmse:445.14382\tvalidation-rmse:537.46343\n",
      "[41]\ttrain-rmse:443.89078\tvalidation-rmse:536.94474\n",
      "[42]\ttrain-rmse:441.95842\tvalidation-rmse:536.62824\n",
      "[43]\ttrain-rmse:439.02561\tvalidation-rmse:536.34776\n",
      "[44]\ttrain-rmse:438.84876\tvalidation-rmse:536.21622\n",
      "[45]\ttrain-rmse:437.20050\tvalidation-rmse:536.13648\n",
      "[46]\ttrain-rmse:435.60291\tvalidation-rmse:536.00412\n",
      "[47]\ttrain-rmse:433.93693\tvalidation-rmse:535.92169\n",
      "[48]\ttrain-rmse:432.45181\tvalidation-rmse:535.80500\n",
      "[49]\ttrain-rmse:429.98802\tvalidation-rmse:536.19361\n",
      "[50]\ttrain-rmse:429.14528\tvalidation-rmse:535.99830\n",
      "[51]\ttrain-rmse:428.27700\tvalidation-rmse:536.07844\n",
      "[52]\ttrain-rmse:426.07167\tvalidation-rmse:536.51502\n",
      "[53]\ttrain-rmse:425.59291\tvalidation-rmse:536.75872\n",
      "[54]\ttrain-rmse:424.09717\tvalidation-rmse:536.90011\n",
      "[55]\ttrain-rmse:423.94615\tvalidation-rmse:536.85423\n",
      "[56]\ttrain-rmse:423.14008\tvalidation-rmse:537.35333\n",
      "[57]\ttrain-rmse:422.88931\tvalidation-rmse:537.16595\n",
      "[58]\ttrain-rmse:421.59988\tvalidation-rmse:536.76630\n",
      "[59]\ttrain-rmse:420.17661\tvalidation-rmse:536.43365\n",
      "[60]\ttrain-rmse:419.25299\tvalidation-rmse:536.32000\n",
      "[61]\ttrain-rmse:418.56206\tvalidation-rmse:536.05820\n",
      "[62]\ttrain-rmse:418.46302\tvalidation-rmse:536.00058\n",
      "[63]\ttrain-rmse:416.61500\tvalidation-rmse:535.75964\n",
      "[64]\ttrain-rmse:415.25238\tvalidation-rmse:536.04760\n",
      "[65]\ttrain-rmse:413.86123\tvalidation-rmse:536.07697\n",
      "[66]\ttrain-rmse:412.24495\tvalidation-rmse:536.63200\n",
      "[67]\ttrain-rmse:411.26513\tvalidation-rmse:536.39088\n",
      "[68]\ttrain-rmse:408.62491\tvalidation-rmse:535.92926\n",
      "[69]\ttrain-rmse:407.31087\tvalidation-rmse:535.67906\n",
      "[70]\ttrain-rmse:406.55028\tvalidation-rmse:535.52289\n",
      "[71]\ttrain-rmse:405.19520\tvalidation-rmse:535.48598\n",
      "[72]\ttrain-rmse:404.85424\tvalidation-rmse:535.32830\n",
      "[73]\ttrain-rmse:403.53415\tvalidation-rmse:535.45477\n",
      "[74]\ttrain-rmse:402.60461\tvalidation-rmse:535.33055\n",
      "[75]\ttrain-rmse:401.02334\tvalidation-rmse:535.77671\n",
      "[76]\ttrain-rmse:399.33433\tvalidation-rmse:535.90100\n",
      "[77]\ttrain-rmse:398.31242\tvalidation-rmse:536.36998\n",
      "[78]\ttrain-rmse:398.11307\tvalidation-rmse:536.26744\n",
      "[79]\ttrain-rmse:397.58961\tvalidation-rmse:536.21778\n",
      "[80]\ttrain-rmse:395.70847\tvalidation-rmse:535.93482\n",
      "[81]\ttrain-rmse:394.27132\tvalidation-rmse:536.10396\n",
      "[82]\ttrain-rmse:391.95238\tvalidation-rmse:536.18454\n",
      "[83]\ttrain-rmse:390.83526\tvalidation-rmse:536.11521\n",
      "[84]\ttrain-rmse:390.20276\tvalidation-rmse:536.16803\n",
      "[85]\ttrain-rmse:390.14086\tvalidation-rmse:536.17118\n",
      "[86]\ttrain-rmse:390.02576\tvalidation-rmse:536.08775\n",
      "[87]\ttrain-rmse:389.97256\tvalidation-rmse:536.06720\n",
      "[88]\ttrain-rmse:389.08055\tvalidation-rmse:535.81676\n",
      "[89]\ttrain-rmse:387.55639\tvalidation-rmse:535.62359\n",
      "[90]\ttrain-rmse:386.14907\tvalidation-rmse:535.82348\n",
      "[91]\ttrain-rmse:385.03249\tvalidation-rmse:535.93612\n",
      "[92]\ttrain-rmse:384.53978\tvalidation-rmse:536.08173\n",
      "[93]\ttrain-rmse:383.26660\tvalidation-rmse:536.24798\n",
      "[94]\ttrain-rmse:383.03980\tvalidation-rmse:536.30422\n",
      "[95]\ttrain-rmse:381.43269\tvalidation-rmse:537.16409\n",
      "[96]\ttrain-rmse:381.19407\tvalidation-rmse:537.33018\n",
      "[97]\ttrain-rmse:379.69258\tvalidation-rmse:537.41471\n",
      "[98]\ttrain-rmse:379.54474\tvalidation-rmse:537.34487\n",
      "[99]\ttrain-rmse:377.16085\tvalidation-rmse:537.66458\n"
     ]
    }
   ],
   "source": [
    "### create evals object\n",
    "evals = [(dtrain_reg, \"train\"), (dtest_reg, \"validation\")] ## note that this comes all the way back from block 10 \n",
    "         \n",
    "model = xgb.train(\n",
    "   params=params,\n",
    "   dtrain=dtrain_reg, \n",
    "   num_boost_round=n,\n",
    "   evals=evals,\n",
    ")\n",
    "  # this now trains and validates the diff models; we should see that the constant testing and applications eventually see\n",
    "    # the RMSE converge, indicating a decent model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec9fe787",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Next step: learning how to divide data into k sets of training and tests. Use k-1 parts for training, and the final as a \n",
    "# test. This process is known as cross validation \n",
    "params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"gpu_hist\"}\n",
    "n = 1000\n",
    "\n",
    "results = xgb.cv(\n",
    "   params, dtrain_reg,\n",
    "   num_boost_round=n,\n",
    "   nfold=5, # how the training sets should be split \n",
    "   early_stopping_rounds=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f498c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-rmse-mean</th>\n",
       "      <th>train-rmse-std</th>\n",
       "      <th>test-rmse-mean</th>\n",
       "      <th>test-rmse-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3973.801938</td>\n",
       "      <td>6.178437</td>\n",
       "      <td>3975.116199</td>\n",
       "      <td>24.759975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2840.612707</td>\n",
       "      <td>5.127063</td>\n",
       "      <td>2846.522997</td>\n",
       "      <td>17.843194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2055.085239</td>\n",
       "      <td>4.299456</td>\n",
       "      <td>2061.095580</td>\n",
       "      <td>14.777246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1516.266259</td>\n",
       "      <td>5.016510</td>\n",
       "      <td>1528.015849</td>\n",
       "      <td>14.213172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1153.500382</td>\n",
       "      <td>5.265913</td>\n",
       "      <td>1171.813627</td>\n",
       "      <td>14.509073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>913.280525</td>\n",
       "      <td>4.866743</td>\n",
       "      <td>937.664571</td>\n",
       "      <td>15.691096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>759.226974</td>\n",
       "      <td>6.034956</td>\n",
       "      <td>789.734256</td>\n",
       "      <td>16.221961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>663.710519</td>\n",
       "      <td>5.496933</td>\n",
       "      <td>700.521848</td>\n",
       "      <td>18.020397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>605.592507</td>\n",
       "      <td>6.255761</td>\n",
       "      <td>648.288943</td>\n",
       "      <td>19.340346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>569.525451</td>\n",
       "      <td>5.841636</td>\n",
       "      <td>617.047260</td>\n",
       "      <td>20.969303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>548.012055</td>\n",
       "      <td>6.411380</td>\n",
       "      <td>599.079515</td>\n",
       "      <td>22.315228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>534.372802</td>\n",
       "      <td>5.558762</td>\n",
       "      <td>587.239778</td>\n",
       "      <td>24.221239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>524.729740</td>\n",
       "      <td>5.442579</td>\n",
       "      <td>580.039955</td>\n",
       "      <td>24.069728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>516.752271</td>\n",
       "      <td>4.805078</td>\n",
       "      <td>575.378326</td>\n",
       "      <td>24.519399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>510.347258</td>\n",
       "      <td>5.490082</td>\n",
       "      <td>570.568110</td>\n",
       "      <td>24.597884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>504.069105</td>\n",
       "      <td>5.918216</td>\n",
       "      <td>567.837084</td>\n",
       "      <td>25.611002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>500.157350</td>\n",
       "      <td>6.556141</td>\n",
       "      <td>565.916189</td>\n",
       "      <td>25.591600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>494.279855</td>\n",
       "      <td>5.032803</td>\n",
       "      <td>563.906778</td>\n",
       "      <td>25.993694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>489.423938</td>\n",
       "      <td>5.794623</td>\n",
       "      <td>563.015111</td>\n",
       "      <td>25.896335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>485.815425</td>\n",
       "      <td>5.957918</td>\n",
       "      <td>562.072161</td>\n",
       "      <td>26.064015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>481.744266</td>\n",
       "      <td>5.317571</td>\n",
       "      <td>561.301828</td>\n",
       "      <td>26.773313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>478.050983</td>\n",
       "      <td>4.746542</td>\n",
       "      <td>560.732951</td>\n",
       "      <td>27.389130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>475.393402</td>\n",
       "      <td>5.007251</td>\n",
       "      <td>560.377485</td>\n",
       "      <td>28.038387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>473.053047</td>\n",
       "      <td>5.399218</td>\n",
       "      <td>559.556988</td>\n",
       "      <td>27.776182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>469.637742</td>\n",
       "      <td>5.076671</td>\n",
       "      <td>558.777503</td>\n",
       "      <td>28.248572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>466.876820</td>\n",
       "      <td>5.431519</td>\n",
       "      <td>558.339463</td>\n",
       "      <td>28.339578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>463.321298</td>\n",
       "      <td>4.940846</td>\n",
       "      <td>557.660710</td>\n",
       "      <td>28.525519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>460.783970</td>\n",
       "      <td>4.682348</td>\n",
       "      <td>557.627643</td>\n",
       "      <td>28.805928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>458.708139</td>\n",
       "      <td>4.876756</td>\n",
       "      <td>557.617869</td>\n",
       "      <td>29.371841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>456.472343</td>\n",
       "      <td>5.543110</td>\n",
       "      <td>557.970352</td>\n",
       "      <td>29.796688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>453.519406</td>\n",
       "      <td>4.775463</td>\n",
       "      <td>557.586115</td>\n",
       "      <td>29.439310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>451.440897</td>\n",
       "      <td>5.086299</td>\n",
       "      <td>557.411323</td>\n",
       "      <td>29.086718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>449.193063</td>\n",
       "      <td>5.781735</td>\n",
       "      <td>556.861486</td>\n",
       "      <td>28.433448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>447.296010</td>\n",
       "      <td>4.429376</td>\n",
       "      <td>556.469515</td>\n",
       "      <td>28.222805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>445.265085</td>\n",
       "      <td>4.829541</td>\n",
       "      <td>556.457471</td>\n",
       "      <td>28.344787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>442.446277</td>\n",
       "      <td>4.821473</td>\n",
       "      <td>555.905803</td>\n",
       "      <td>27.872267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>440.261354</td>\n",
       "      <td>4.802247</td>\n",
       "      <td>555.740607</td>\n",
       "      <td>28.300005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>438.187072</td>\n",
       "      <td>4.953618</td>\n",
       "      <td>555.258871</td>\n",
       "      <td>28.559559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>435.863927</td>\n",
       "      <td>4.946029</td>\n",
       "      <td>555.475880</td>\n",
       "      <td>28.676298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>434.483392</td>\n",
       "      <td>5.395065</td>\n",
       "      <td>555.459798</td>\n",
       "      <td>28.479151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>432.650321</td>\n",
       "      <td>5.477219</td>\n",
       "      <td>555.549633</td>\n",
       "      <td>28.528278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>431.385549</td>\n",
       "      <td>5.922425</td>\n",
       "      <td>555.070110</td>\n",
       "      <td>28.378379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>430.151121</td>\n",
       "      <td>5.411159</td>\n",
       "      <td>554.764496</td>\n",
       "      <td>27.975556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>428.596000</td>\n",
       "      <td>6.318850</td>\n",
       "      <td>554.465529</td>\n",
       "      <td>27.848214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
       "0       3973.801938        6.178437     3975.116199      24.759975\n",
       "1       2840.612707        5.127063     2846.522997      17.843194\n",
       "2       2055.085239        4.299456     2061.095580      14.777246\n",
       "3       1516.266259        5.016510     1528.015849      14.213172\n",
       "4       1153.500382        5.265913     1171.813627      14.509073\n",
       "5        913.280525        4.866743      937.664571      15.691096\n",
       "6        759.226974        6.034956      789.734256      16.221961\n",
       "7        663.710519        5.496933      700.521848      18.020397\n",
       "8        605.592507        6.255761      648.288943      19.340346\n",
       "9        569.525451        5.841636      617.047260      20.969303\n",
       "10       548.012055        6.411380      599.079515      22.315228\n",
       "11       534.372802        5.558762      587.239778      24.221239\n",
       "12       524.729740        5.442579      580.039955      24.069728\n",
       "13       516.752271        4.805078      575.378326      24.519399\n",
       "14       510.347258        5.490082      570.568110      24.597884\n",
       "15       504.069105        5.918216      567.837084      25.611002\n",
       "16       500.157350        6.556141      565.916189      25.591600\n",
       "17       494.279855        5.032803      563.906778      25.993694\n",
       "18       489.423938        5.794623      563.015111      25.896335\n",
       "19       485.815425        5.957918      562.072161      26.064015\n",
       "20       481.744266        5.317571      561.301828      26.773313\n",
       "21       478.050983        4.746542      560.732951      27.389130\n",
       "22       475.393402        5.007251      560.377485      28.038387\n",
       "23       473.053047        5.399218      559.556988      27.776182\n",
       "24       469.637742        5.076671      558.777503      28.248572\n",
       "25       466.876820        5.431519      558.339463      28.339578\n",
       "26       463.321298        4.940846      557.660710      28.525519\n",
       "27       460.783970        4.682348      557.627643      28.805928\n",
       "28       458.708139        4.876756      557.617869      29.371841\n",
       "29       456.472343        5.543110      557.970352      29.796688\n",
       "30       453.519406        4.775463      557.586115      29.439310\n",
       "31       451.440897        5.086299      557.411323      29.086718\n",
       "32       449.193063        5.781735      556.861486      28.433448\n",
       "33       447.296010        4.429376      556.469515      28.222805\n",
       "34       445.265085        4.829541      556.457471      28.344787\n",
       "35       442.446277        4.821473      555.905803      27.872267\n",
       "36       440.261354        4.802247      555.740607      28.300005\n",
       "37       438.187072        4.953618      555.258871      28.559559\n",
       "38       435.863927        4.946029      555.475880      28.676298\n",
       "39       434.483392        5.395065      555.459798      28.479151\n",
       "40       432.650321        5.477219      555.549633      28.528278\n",
       "41       431.385549        5.922425      555.070110      28.378379\n",
       "42       430.151121        5.411159      554.764496      27.975556\n",
       "43       428.596000        6.318850      554.465529      27.848214"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results # take a look at the output; note that the rmses are not sig diff near the end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b615680e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "554.4655285341859"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### look at best rmse \n",
    "best_rmse = results['test-rmse-mean'].min()\n",
    "\n",
    "best_rmse # which is the last of course "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6eb675ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### xgboost classifications - refers to what the dv will be (i.e. model type). For general categories, these can take form of \n",
    "# binary:logistic  and multi:softprob ; note that the later is multinomial, not ordinal. \n",
    "## THe next code is going to be of the multinomial variety \n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "X, y = diamonds.drop(\"cut\", axis=1), diamonds[['cut']] # splits data into X and y, with the X being all variables but cut,\n",
    "# with y being the cut data. \n",
    "\n",
    "# Encode y to numeric\n",
    "y_encoded = OrdinalEncoder().fit_transform(y) # which will then be read as factors and such \n",
    "\n",
    "# Extract text features\n",
    "cats = X.select_dtypes(exclude=np.number).columns.tolist() ## also present above \n",
    "\n",
    "# Convert to pd.Categorical\n",
    "for col in cats:\n",
    "   X[col] = X[col].astype('category') # this gets the code to be more efficient as factors \n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, random_state=1, stratify=y_encoded)\n",
    "# this code splits again into the train and test sets, with the stuff in parentheses fed into the 4 objects before the \"=\"\n",
    "# stratify is a useful command, ensuring that there is a proportionate amount of the y in all of the data sets, i.e. no \n",
    "# data set is simply all 1s or what have you "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ddee7644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ideal', 'Premium', 'Good', 'Very Good', 'Fair']\n",
       "Categories (5, object): ['Ideal', 'Premium', 'Very Good', 'Good', 'Fair']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['cut'].unique() # take a look at the types of categories; note that there are 5, and will need to be fed in below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c7608a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification matrices; these will be fed into the xgb code below, and are xgb objects \n",
    "dtrain_clf = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "dtest_clf = xgb.DMatrix(X_test, y_test, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b858ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"objective\": \"multi:softprob\", \"tree_method\": \"gpu_hist\", \"num_class\": 5}\n",
    "n = 1000\n",
    "\n",
    "results = xgb.cv(\n",
    "   params, dtrain_clf,\n",
    "   num_boost_round=n,\n",
    "   nfold=5,\n",
    "   metrics=[\"mlogloss\", \"auc\", \"merror\"], ## here we are telling to look at three metrics, which are \n",
    "    # multi-class log loss, area under the ROC curve, and multi-class classification error\n",
    ") # note that the auc is the flase positive rate (x axis) plotted against the true positive rate (y-axis). Closer to 1, teh \n",
    "# better, with 0 being garbage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0af64af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['train-mlogloss-mean', 'train-mlogloss-std', 'train-auc-mean',\n",
       "       'train-auc-std', 'train-merror-mean', 'train-merror-std',\n",
       "       'test-mlogloss-mean', 'test-mlogloss-std', 'test-auc-mean',\n",
       "       'test-auc-std', 'test-merror-mean', 'test-merror-std'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys() # keys() method returns a list of all the keys in a dictionary, and what can be analyzed. Let's take a look\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77e91471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9402233623451636"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['test-auc-mean'].max()# at 0.94, pretty good. We'll want to run similar tests for our data. \n",
    "# what we would do is essentially whats going on here, though we'd do something ordinal \n",
    "# https://analyticsindiamag.com/a-complete-tutorial-on-ordinal-regression-in-python/\n",
    "# in the event that the code doesn't match up, we can essentially first predict if something is somewhat toxic at least, and \n",
    "# then from there, just predict if its fully toxic. This would simply be running the logit twice. Easy enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9fd927bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Train a model using the scikit-learn API\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=100, objective='binary:logistic', tree_method='hist', eta=0.1, max_depth=3,\n",
    "                                  enable_categorical=True)\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Convert the model to a native API model; this is what we can do to export our model and test on other data sets \n",
    "model = xgb_classifier.get_booster() ## mow we should be able to apply this elsewhere. Let's see "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "14655afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xgboost.core.Booster at 0x1de49046d00>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7dfeeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### to apply, we should only need the X data, with y then being predicted. Let's try that .\n",
    "\n",
    "x_predict_pre = diamonds.drop(\"cut\", axis=1)\n",
    "\n",
    "# Extract text features\n",
    "cats = x_predict_pre.select_dtypes(exclude=np.number).columns.tolist() ## also present above \n",
    "\n",
    "# Convert to pd.Categorical\n",
    "for col in cats:\n",
    "   x_predict_pre[col] = x_predict_pre[col].astype('category') # this gets the code to be more efficient as factors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "944fbe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## note, to apply we still need everything to be a matrix in the xgb manner. we should be able to call that \n",
    "x_predict_pre_mat = xgb.DMatrix(x_predict_pre, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec01b242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.2803692e-04, 4.5760651e-03, 7.4318254e-01, 2.2490121e-02,\n",
       "        2.2892323e-01],\n",
       "       [1.1407522e-03, 7.7159353e-02, 8.7511810e-03, 4.9219695e-01,\n",
       "        4.2075172e-01],\n",
       "       [1.6888994e-01, 6.8149847e-01, 1.4987147e-03, 1.3883917e-03,\n",
       "        1.4672449e-01],\n",
       "       ...,\n",
       "       [3.9300360e-03, 2.8588120e-02, 1.6254615e-02, 4.6576676e-01,\n",
       "        4.8546046e-01],\n",
       "       [2.6861599e-03, 1.0294575e-02, 9.3398370e-02, 6.2612146e-01,\n",
       "        2.6749945e-01],\n",
       "       [1.3444772e-03, 4.9832766e-03, 8.4948701e-01, 5.9855450e-02,\n",
       "        8.4329791e-02]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### now let's apply having cleaned it \n",
    "x_predict_post = model.predict(x_predict_pre_mat)\n",
    "x_predict_post ## this worked!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8e360355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ideal_pr</th>\n",
       "      <th>Premium_pr</th>\n",
       "      <th>Very_Good_pr</th>\n",
       "      <th>Good_pr</th>\n",
       "      <th>Fair_pr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.004576</td>\n",
       "      <td>0.743183</td>\n",
       "      <td>0.022490</td>\n",
       "      <td>0.228923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.077159</td>\n",
       "      <td>0.008751</td>\n",
       "      <td>0.492197</td>\n",
       "      <td>0.420752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.168890</td>\n",
       "      <td>0.681498</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.001388</td>\n",
       "      <td>0.146724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.006635</td>\n",
       "      <td>0.068685</td>\n",
       "      <td>0.266934</td>\n",
       "      <td>0.656605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.680335</td>\n",
       "      <td>0.008914</td>\n",
       "      <td>0.002010</td>\n",
       "      <td>0.307471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53935</th>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.007377</td>\n",
       "      <td>0.763904</td>\n",
       "      <td>0.087403</td>\n",
       "      <td>0.139326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53936</th>\n",
       "      <td>0.002902</td>\n",
       "      <td>0.231594</td>\n",
       "      <td>0.037354</td>\n",
       "      <td>0.003894</td>\n",
       "      <td>0.724256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53937</th>\n",
       "      <td>0.003930</td>\n",
       "      <td>0.028588</td>\n",
       "      <td>0.016255</td>\n",
       "      <td>0.465767</td>\n",
       "      <td>0.485460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53938</th>\n",
       "      <td>0.002686</td>\n",
       "      <td>0.010295</td>\n",
       "      <td>0.093398</td>\n",
       "      <td>0.626121</td>\n",
       "      <td>0.267499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53939</th>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.004983</td>\n",
       "      <td>0.849487</td>\n",
       "      <td>0.059855</td>\n",
       "      <td>0.084330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53940 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Ideal_pr  Premium_pr  Very_Good_pr   Good_pr   Fair_pr\n",
       "0      0.000828    0.004576      0.743183  0.022490  0.228923\n",
       "1      0.001141    0.077159      0.008751  0.492197  0.420752\n",
       "2      0.168890    0.681498      0.001499  0.001388  0.146724\n",
       "3      0.001142    0.006635      0.068685  0.266934  0.656605\n",
       "4      0.001270    0.680335      0.008914  0.002010  0.307471\n",
       "...         ...         ...           ...       ...       ...\n",
       "53935  0.001990    0.007377      0.763904  0.087403  0.139326\n",
       "53936  0.002902    0.231594      0.037354  0.003894  0.724256\n",
       "53937  0.003930    0.028588      0.016255  0.465767  0.485460\n",
       "53938  0.002686    0.010295      0.093398  0.626121  0.267499\n",
       "53939  0.001344    0.004983      0.849487  0.059855  0.084330\n",
       "\n",
       "[53940 rows x 5 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### now lets get this into a data frame \n",
    "col_names = ['Ideal_pr', 'Premium_pr', 'Very_Good_pr', 'Good_pr', 'Fair_pr'] \n",
    "col_names = \n",
    "x_predict_post_df = pd.DataFrame(x_predict_post, columns=col_names)\n",
    "x_predict_post_df ## see that this gets us the probs that it is part of a given category \n",
    "## now from here, I'll simply want to rename based upon y, which are \n",
    "# ['Ideal', 'Premium', 'Very Good', 'Good', 'Fair'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "410e7469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_predict_post_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4d3f5fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### another helpful link: https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/\n",
    "## now bind \n",
    "full_df = pd.concat([diamonds, x_predict_post_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "565b4f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>Ideal_pr</th>\n",
       "      <th>Premium_pr</th>\n",
       "      <th>Very_Good_pr</th>\n",
       "      <th>Good_pr</th>\n",
       "      <th>Fair_pr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.004576</td>\n",
       "      <td>0.743183</td>\n",
       "      <td>0.022490</td>\n",
       "      <td>0.228923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.077159</td>\n",
       "      <td>0.008751</td>\n",
       "      <td>0.492197</td>\n",
       "      <td>0.420752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.168890</td>\n",
       "      <td>0.681498</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.001388</td>\n",
       "      <td>0.146724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.006635</td>\n",
       "      <td>0.068685</td>\n",
       "      <td>0.266934</td>\n",
       "      <td>0.656605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.680335</td>\n",
       "      <td>0.008914</td>\n",
       "      <td>0.002010</td>\n",
       "      <td>0.307471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53935</th>\n",
       "      <td>0.72</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>60.8</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.75</td>\n",
       "      <td>5.76</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.007377</td>\n",
       "      <td>0.763904</td>\n",
       "      <td>0.087403</td>\n",
       "      <td>0.139326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53936</th>\n",
       "      <td>0.72</td>\n",
       "      <td>Good</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>63.1</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.69</td>\n",
       "      <td>5.75</td>\n",
       "      <td>3.61</td>\n",
       "      <td>0.002902</td>\n",
       "      <td>0.231594</td>\n",
       "      <td>0.037354</td>\n",
       "      <td>0.003894</td>\n",
       "      <td>0.724256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53937</th>\n",
       "      <td>0.70</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>62.8</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.66</td>\n",
       "      <td>5.68</td>\n",
       "      <td>3.56</td>\n",
       "      <td>0.003930</td>\n",
       "      <td>0.028588</td>\n",
       "      <td>0.016255</td>\n",
       "      <td>0.465767</td>\n",
       "      <td>0.485460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53938</th>\n",
       "      <td>0.86</td>\n",
       "      <td>Premium</td>\n",
       "      <td>H</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>6.15</td>\n",
       "      <td>6.12</td>\n",
       "      <td>3.74</td>\n",
       "      <td>0.002686</td>\n",
       "      <td>0.010295</td>\n",
       "      <td>0.093398</td>\n",
       "      <td>0.626121</td>\n",
       "      <td>0.267499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53939</th>\n",
       "      <td>0.75</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>D</td>\n",
       "      <td>SI2</td>\n",
       "      <td>62.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.83</td>\n",
       "      <td>5.87</td>\n",
       "      <td>3.64</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.004983</td>\n",
       "      <td>0.849487</td>\n",
       "      <td>0.059855</td>\n",
       "      <td>0.084330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53940 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       carat        cut color clarity  depth  table  price     x     y     z  \\\n",
       "0       0.23      Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43   \n",
       "1       0.21    Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31   \n",
       "2       0.23       Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31   \n",
       "3       0.29    Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63   \n",
       "4       0.31       Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75   \n",
       "...      ...        ...   ...     ...    ...    ...    ...   ...   ...   ...   \n",
       "53935   0.72      Ideal     D     SI1   60.8   57.0   2757  5.75  5.76  3.50   \n",
       "53936   0.72       Good     D     SI1   63.1   55.0   2757  5.69  5.75  3.61   \n",
       "53937   0.70  Very Good     D     SI1   62.8   60.0   2757  5.66  5.68  3.56   \n",
       "53938   0.86    Premium     H     SI2   61.0   58.0   2757  6.15  6.12  3.74   \n",
       "53939   0.75      Ideal     D     SI2   62.2   55.0   2757  5.83  5.87  3.64   \n",
       "\n",
       "       Ideal_pr  Premium_pr  Very_Good_pr   Good_pr   Fair_pr  \n",
       "0      0.000828    0.004576      0.743183  0.022490  0.228923  \n",
       "1      0.001141    0.077159      0.008751  0.492197  0.420752  \n",
       "2      0.168890    0.681498      0.001499  0.001388  0.146724  \n",
       "3      0.001142    0.006635      0.068685  0.266934  0.656605  \n",
       "4      0.001270    0.680335      0.008914  0.002010  0.307471  \n",
       "...         ...         ...           ...       ...       ...  \n",
       "53935  0.001990    0.007377      0.763904  0.087403  0.139326  \n",
       "53936  0.002902    0.231594      0.037354  0.003894  0.724256  \n",
       "53937  0.003930    0.028588      0.016255  0.465767  0.485460  \n",
       "53938  0.002686    0.010295      0.093398  0.626121  0.267499  \n",
       "53939  0.001344    0.004983      0.849487  0.059855  0.084330  \n",
       "\n",
       "[53940 rows x 15 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92a6257",
   "metadata": {},
   "outputs": [],
   "source": [
    "### next step: try the predictions on text data "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
